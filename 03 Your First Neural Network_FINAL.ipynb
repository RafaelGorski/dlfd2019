{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\local\\envs\\deepfuture\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\local\\envs\\deepfuture\\lib\\site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\local\\envs\\deepfuture\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\local\\envs\\deepfuture\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\local\\envs\\deepfuture\\lib\\site-packages (from scikit-learn->sklearn) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature records:  150\n",
      "\n",
      "First five records: [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# I created a special python file that's full of helpers for our notebooks. \n",
    "# If you are code inclined, feel free to look inside the file and see what I'm doing there\n",
    "# for others, this Helper file will just make some functions easier to handle for beginners\n",
    "import Helpers\n",
    "\n",
    "# Specifically, we want the helper for this Iris project\n",
    "helper = Helpers.Iris\n",
    "\n",
    "# sklearn comes with the whole Iris Dataset, so let's make sure to load it\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "\n",
    "flower_names = [\"SETOSA\", \"VERSICOLOR\", \"VIRGINICA\"]\n",
    "total_classes = len(flower_names)\n",
    "\n",
    "print(\"Total feature records: \", len(features))\n",
    "print(\"\\nFirst five records:\", features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 150\n",
      "\n",
      "All label records:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"total labels:\", len(labels))\n",
    "print(\"\\nAll label records: \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the records are sorted by type of flower. For training, we would rather have our data shuffled around, so we will load some helpers functions to shuffle them around and then verify that the order is now random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature records:  150\n",
      "Total labels:  150\n",
      "\n",
      "All label records:  [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0 1 2 2 1 2 1 2 1 0 2 1 0 0 0 1 2 0 0 0 1 0 1 2 0 1 2 0 2 2\n",
      " 1 1 2 1 0 1 2 0 0 1 1 0 2 0 0 1 1 2 1 2 2 1 0 0 2 2 0 0 0 1 2 0 2 2 0 1 1\n",
      " 2 1 2 0 2 1 2 1 1 1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0\n",
      " 1 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features, labels = helper.shuffle_records(features, labels)\n",
    "\n",
    "print(\"Total feature records: \", len(features))\n",
    "print(\"Total labels: \", len(labels))\n",
    "print(\"\\nAll label records: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original first 5 labels: [1 0 2 1 1]\n",
      "\n",
      "First 5 one hot labels: [[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_labels = helper.convert_to_one_hot(labels, total_classes)\n",
    "\n",
    "print(\"the original first 5 labels:\", labels[:5])\n",
    "print(\"\\nFirst 5 one hot labels:\", one_hot_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE NEURAL NET #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\local\\envs\\deepfuture\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def TrainingLoop(number_of_epochs, verbose):\n",
    "    model.fit(features,\n",
    "             one_hot_labels,\n",
    "             batch_size=batch_size,\n",
    "             epochs=number_of_epochs,\n",
    "             verbose=verbose,\n",
    "             validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\local\\envs\\deepfuture\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 135 samples, validate on 15 samples\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 2s 12ms/step - loss: 2.8467 - acc: 0.3556 - val_loss: 3.7240 - val_acc: 0.1333\n"
     ]
    }
   ],
   "source": [
    "TrainingLoop(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135 samples, validate on 15 samples\n",
      "Epoch 1/1\n",
      "135/135 [==============================] - 0s 37us/step - loss: 0.2973 - acc: 0.9556 - val_loss: 0.3496 - val_acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "TrainingLoop(500, 0)\n",
    "TrainingLoop(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135 samples, validate on 15 samples\n",
      "Epoch 1/10\n",
      "135/135 [==============================] - 0s 52us/step - loss: 0.2971 - acc: 0.9556 - val_loss: 0.3490 - val_acc: 0.9333\n",
      "Epoch 2/10\n",
      "135/135 [==============================] - 0s 37us/step - loss: 0.2965 - acc: 0.9556 - val_loss: 0.3482 - val_acc: 0.9333\n",
      "Epoch 3/10\n",
      "135/135 [==============================] - 0s 37us/step - loss: 0.2954 - acc: 0.9556 - val_loss: 0.3473 - val_acc: 0.9333\n",
      "Epoch 4/10\n",
      "135/135 [==============================] - 0s 37us/step - loss: 0.2941 - acc: 0.9556 - val_loss: 0.3467 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "135/135 [==============================] - 0s 44us/step - loss: 0.2926 - acc: 0.9630 - val_loss: 0.3464 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "135/135 [==============================] - 0s 37us/step - loss: 0.2916 - acc: 0.9778 - val_loss: 0.3469 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "135/135 [==============================] - 0s 52us/step - loss: 0.2908 - acc: 0.9778 - val_loss: 0.3474 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "135/135 [==============================] - 0s 44us/step - loss: 0.2903 - acc: 0.9704 - val_loss: 0.3476 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "135/135 [==============================] - 0s 52us/step - loss: 0.2900 - acc: 0.9630 - val_loss: 0.3474 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "135/135 [==============================] - 0s 52us/step - loss: 0.2896 - acc: 0.9630 - val_loss: 0.3465 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "TrainingLoop(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.1, 2.8, 4.7, 1.2],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [7.7, 2.6, 6.9, 2.3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.8573643e-01, 1.4195374e-02, 6.8137626e-05]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([ [5.7, 3.8, 1.7, 0.3] ] )\n",
    "result = model.predict( test_data, batch_size=1, verbose=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SETOSA'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_index = np.argmax(result, axis=1)[0]\n",
    "flower_names[highest_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
