{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Files for Class Review\n",
    "\n",
    "**Note:** The following files are provided for reference to follow along with the class discussion. You should not attempt to run the cells below, as the original training data is not for public distribution.\n",
    "\n",
    "The training data the instructor will use locally is found in a directory called faces/, which you can see below. Note that there are 1,259 images we will use as our training data. The ground truth label for each image is embedded in it's file name. Files that begin with \"f_\" will be processed as female and files that begin with \"m_\" will be processed as male. Also, all the images have already been resized to the 120x100 size we will expect from the model we will build.\n",
    "\n",
    "*These images originally come from public websites containing images of various countries members of congress.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My image folder to use as training data](images/faces_peek.png)\n",
    "\n",
    "*What the \"faces/\" directory on my machine looks like.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Train On Faces.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "image_files_on_disk = []\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# STEP 1 - COLLECT ALL THE FILE NAMES IN THE \n",
    "# IMAGE FOLDER THAT HAS YOUR TRAINING DATA\n",
    "for file in os.listdir(\"faces\"):\n",
    "    image_files_on_disk.append(file)\n",
    "\n",
    "random.shuffle(image_files_on_disk)\n",
    "\n",
    "# STEP 2 - FOR EACH FILE NAME, LOAD THE IMAGE,\n",
    "# CONVERT IT TO AN ARRAY AND STORE IT IN OUR DATA ARRAY\n",
    "# THE FILE NAME CONTAINS THE GENDER ASSIGNMENT WE WILL\n",
    "# USE AS GROUND TRUTH. FILES BEGIN WITH EITHER \"m_\"\n",
    "# OR \"f_\". IF THE FILE STARTS WITH \"f_\" WE WILL ASSIGN\n",
    "# IT A LABEL OF 1 (female). OTHERWISE, WE WILL ASSUME 0 (male)\n",
    "for file_name in image_files_on_disk:\n",
    "    image = cv2.imread(\"faces/\" + file_name)\n",
    "    data.append( img_to_array(image))\n",
    "    label = 0\n",
    "    if file_name.startswith(\"f_\"):\n",
    "        label = 1\n",
    "    labels.append([label])\n",
    "\n",
    "# CONVERT OUR DATA AND LABELS TO NUMPY ARRAYS\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ONE HOT ENCODE OUR LABELS\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# STEP 3 - DEFINE OUR CNN MODEL\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=(120, 100, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# DEFINE OUR OPTIMIZER\n",
    "opt = Adam(lr=lr, decay=lr/epochs)\n",
    "\n",
    "# SINCE WE ONLY HAVE TWO CATEGORIES, WE COMPILE\n",
    "# OUR MODEL AS A BINARY CLASSFIER\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# ONCE OUR MODEL IS COMPILED, WE TRAIN IT (fit)\n",
    "# BY THE NUMBER OF EPOCHS WE DEFINED ABOVE\n",
    "model.fit(data, labels,batch_size=batch_size, epochs=epochs, verbose=1 )\n",
    "\n",
    "# ONCE WE ARE DONE, WE CAN SAVE THE MODEL AS FOLLOWS\n",
    "model.save(\"face100.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Webcam test faces.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# STEP 1: LOAD THE MODEL FOR PREDICTION\n",
    "model = load_model(\"face100.model\")\n",
    "# DEFINE HUMAN READABLE CLASS NAMES, SINCE THE\n",
    "# MODEL PREDICTION WILL RETURN PROBABILITIES FOR\n",
    "# 0 (male) AND 1 (female)\n",
    "classes = [\"man\", \"woman\"]\n",
    "\n",
    "# STEP 2: START WEBCAM\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"can not access webcam\")\n",
    "    exit()\n",
    "\n",
    "# CONTINUOUSLY LOOP...\n",
    "while webcam.isOpened():\n",
    "\n",
    "    _, frame = webcam.read()\n",
    "    # MY LOCAL WEBCAM IS 640x480 SO I'M ASSUMING\n",
    "    # THE ABILITY TO CLIP A 400x480 BOX FROM THE MIDDLE\n",
    "    # I REWRITE THIS TO MAKE IT USABLE FOR ANY WEBCAM LATER\n",
    "    image = frame[0:640, 120:520]\n",
    "\n",
    "    # NOW WE NEED TO GET OUR IMAGE READY TO BE\n",
    "    # SUBMITTED TO OUR TRAINED MODEL FOR TESTING.\n",
    "    # WE RESIZE, CONVERT TO FLOAT, AND CREATE \n",
    "    # A SINGLE IMAGE BATCH TO FEED INTO OUR MODEL\n",
    "    tester = cv2.resize(image, (100, 120))\n",
    "    tester = tester.astype(\"float\") / 255.0\n",
    "    tester = img_to_array(tester)\n",
    "    tester = np.expand_dims(tester, axis=0)\n",
    "\n",
    "    # WE NOW FEED IT TO OUR MODEL TO GET BACK OUR GENDER PREDICTIONS\n",
    "    conf = model.predict(tester)[0]\n",
    "\n",
    "    # WE CONVERT THE HIGHEST PREDICTION INTO A HUMAN READABLE LABEL\n",
    "    idx = np.argmax(conf)\n",
    "    label = classes[idx]\n",
    "\n",
    "    # AND DRAW IT DIRECTLY INTO OUR WEBCAM IMAGE\n",
    "    label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n",
    "    cv2.putText(image, label, (10, 440), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "    \n",
    "    # WE THEN SHOW THE UPDATED WEBCAM IMAGE\n",
    "    cv2.imshow(\"gender detection\", image)\n",
    "\n",
    "    # EXIT BY PRESSING 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results from the first test](images/02_webcam_result.png)\n",
    "\n",
    "*Not Good. We have no idea what the model learned!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Preprocess data to boxedFaces.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# WE WILL USE OPENCV's FACE DETECTOR\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# FOR EACH FILE IN FACES, WE WANT TO LOAD INTO MEMORY\n",
    "# THEN FEED A GREYSCALE COPY INTO OUR FACE DETECTOR.\n",
    "for file_name in os.listdir(\"faces\"):\n",
    "    image = cv2.imread(\"faces/\" + file_name)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(20,20) )\n",
    "\n",
    "    # IF ANY FACES ARE FOUND, BLACK OUT THE TOP, BOTTOM AND SIDES\n",
    "    # TO ONLY LEAVE THE FACE VISIBLE\n",
    "    for ( x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (0,0), (100,y), (0,0,0), -1)\n",
    "        cv2.rectangle(image, (0,0), (x, 120), (0,0,0), -1)\n",
    "        cv2.rectangle(image, (0, y+h), (100,120), (0,0,0), -1)\n",
    "        cv2.rectangle(image, (x+w, 0), (100, 120), (0,0,0), -1)\n",
    "\n",
    "    # SAVE THIS MODIFIED COPY INTO OUR NEW DIRECTORY\n",
    "    newLocation = \"boxedFaces/\" + file_name\n",
    "    cv2.imwrite(newLocation, image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My image folder to use as training data, now preprocessed to only show the face](images/boxedFaces_peek.png)\n",
    "\n",
    "*What the \"boxedFaces/\" directory on my machine looks like. We are not attempting to only show the face*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Train On Boxed Faces.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "image_files_on_disk = []\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "'''\n",
    "--------------------------------------------------------------\n",
    "NOTE: THIS FILE IS ALMOST IDENTICAL TO \"01 Train On Faces.py\"\n",
    "  THE ONLY DIFFERENCE IS THAT IT WILL LOOK AND READ FROM\n",
    "  THE 'boxedFaces/' DIRECTORY INSTEAD OF THE ORIGINAL \n",
    "  'faces/' DIRECTORY. ALSO, IT SHOULD SAVE THE TRAINED\n",
    "  model under the file name \"boxedFace100.model\"\n",
    "  --------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# STEP 1 - COLLECT ALL THE FILE NAMES IN THE \n",
    "# IMAGE FOLDER THAT HAS YOUR TRAINING DATA\n",
    "for file in os.listdir(\"boxedFaces\"):\n",
    "    image_files_on_disk.append(file)\n",
    "\n",
    "random.shuffle(image_files_on_disk)\n",
    "\n",
    "# STEP 2 - FOR EACH FILE NAME, LOAD THE IMAGE,\n",
    "# CONVERT IT TO AN ARRAY AND STORE IT IN OUR DATA ARRAY\n",
    "# THE FILE NAME CONTAINS THE GENDER ASSIGNMENT WE WILL\n",
    "# USE AS GROUND TRUTH. FILES BEGIN WITH EITHER \"m_\"\n",
    "# OR \"f_\". IF THE FILE STARTS WITH \"f_\" WE WILL ASSIGN\n",
    "# IT A LABEL OF 1 (female). OTHERWISE, WE WILL ASSUME 0 (male)\n",
    "for file_name in image_files_on_disk:\n",
    "    image = cv2.imread(\"boxedFaces/\" + file_name)\n",
    "    data.append( img_to_array(image))\n",
    "    label = 0\n",
    "    if file_name.startswith(\"f_\"):\n",
    "        label = 1\n",
    "    labels.append([label])\n",
    "\n",
    "# CONVERT OUR DATA AND LABELS TO NUMPY ARRAYS\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ONE HOT ENCODE OUR LABELS\n",
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# STEP 3 - DEFINE OUR CNN MODEL\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=(120, 100, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# DEFINE AN OPTIMIZER\n",
    "opt = Adam(lr=lr, decay=lr/epochs)\n",
    "\n",
    "# SINCE WE ONLY HAVE TWO CATEGORIES, WE COMPILE\n",
    "# OUR MODEL AS A BINARY CLASSIFIER\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# ONCE OUR MODEL IS COMPILED, WE TRAIN IT (fit)\n",
    "# BY THE NUMBER OF EPOCHS WE DEFINED ABOVE\n",
    "model.fit(data, labels,batch_size=batch_size, epochs=epochs, verbose=1 )\n",
    "\n",
    "# ONCE WE ARE DONE, WE CAN SAVE THE MODEL AS FOLLOWS:\n",
    "model.save(\"boxedFace100.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results from the second test](images/04_webcam_result.png)\n",
    "\n",
    "*It's Getting Better, but it is very jittery. Also, there is significant failure on female subjects. We need more training data...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Train on Boxed Faces AUGMENTED.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "image_files_on_disk = []\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "'''\n",
    "--------------------------------------------------------------\n",
    "NOTE: THIS FILE IS VERY SIMILAR TO \"04 Train On Boxed Faces.py\"\n",
    "  HOWEVER, NOTICE THAT WE ARE USING AN ImageDataGenerator TO\n",
    "  AUGMENT EACH IMAGE FILE IN VARIOUS WAYS (scale, zoom, shearing, etc.)\n",
    "\n",
    "  THIS MEANS THAT WE WILL ALSO CHANGE THE MODEL TRAINING COMMAND\n",
    "  FROM 'model.fit()' TO 'model.fit_generator()' (see below)\n",
    "  --------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# STEP 1 - COLLECT ALL THE FILE NAMES IN THE \n",
    "# IMAGE FOLDER THAT HAS YOUR TRAINING DATA\n",
    "for file in os.listdir(\"boxedFaces\"):\n",
    "    image_files_on_disk.append(file)\n",
    "\n",
    "# STEP 2 - FOR EACH FILE NAME, LOAD THE IMAGE,\n",
    "# CONVERT IT TO AN ARRAY AND STORE IT IN OUR DATA ARRAY\n",
    "# THE FILE NAME CONTAINS THE GENDER ASSIGNMENT WE WILL\n",
    "# USE AS GROUND TRUTH. FILES BEGIN WITH EITHER \"m_\"\n",
    "# OR \"f_\". IF THE FILE STARTS WITH \"f_\" WE WILL ASSIGN\n",
    "# IT A LABEL OF 1 (female). OTHERWISE, WE WILL ASSUME 0 (male)\n",
    "random.shuffle(image_files_on_disk)\n",
    "\n",
    "for file_name in image_files_on_disk:\n",
    "    image = cv2.imread(\"boxedFaces/\" + file_name)\n",
    "    data.append( img_to_array(image))\n",
    "    label = 0\n",
    "    if file_name.startswith(\"f_\"):\n",
    "        label = 1\n",
    "    labels.append([label])\n",
    "\n",
    "# CONVERT OUR DATA AND LABELS TO NUMPY ARRAYS\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "'''\n",
    "# --------------------------------------------------\n",
    "# **NEW**\n",
    "# --------------------------------------------------\n",
    "# WE ARE SPLITTING UP OUR DATA TO CREATE A SMALL AMOUNT\n",
    "# OF TESTING DATA, THIS HELPS WITH IMPROVING ACCURACY\n",
    "# NOTICE THAT data GETS SPLIT INTO 'trainX' and 'testX'\n",
    "# AND THAT labels GETS SPLIT INTO 'trainY' and 'testY'\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# WHICH MEANS WE NOW NEED TO CONVERT BOTH OUR \n",
    "# TESTING AND TRAINING LABELS (trainY, testY) INTO\n",
    "# ONE HOT ENCODED TOO...\n",
    "from keras.utils import to_categorical\n",
    "trainY = to_categorical(trainY, num_classes = 2)\n",
    "testY = to_categorical(testY, num_classes = 2)\n",
    "\n",
    "'''\n",
    "# --------------------------------------------------\n",
    "# **NEW**\n",
    "# --------------------------------------------------\n",
    "# AND HERE IS WHERE WE CREATE THE 'ImageDataGenerator' WHICH\n",
    "# DEFINES HOW WE WANT TO AUGMENT EACH IMAGE DATA (rotation, shifting, shearing, zoom, scale, etc. )\n",
    "# THIS IS A KEY WAY TO TURN A LITTLE BIT OF TRAINING DATA \n",
    "# INTO A LARGER AMOUNT, AS EACH ORIGINAL PICTURE CAN PRODUCE\n",
    "# SEVERAL VARIATIONS, ALL SLIGHTLY DIFFERENT FROM EACH OTHER\n",
    "'''\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "augmentedData = ImageDataGenerator(rotation_range=25, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.3, horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "\n",
    "# STEP 3 - DEFINE OUR CNN MODEL (no change)\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=(120, 100, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# DEFINE OUR OPTIMIZER\n",
    "opt = Adam(lr=lr, decay=lr/epochs)\n",
    "\n",
    "# SINCE WE ONLY HAVE TWO CATEGORIES, WE COMPILE\n",
    "# OUR MODEL AS A BINARY CLASSIFIER\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "'''\n",
    "# --------------------------------------------------\n",
    "# **NEW**\n",
    "# --------------------------------------------------\n",
    "# WE ARE NOT USING JUST model.fit() AS WE DID BEFORE\n",
    "# BUT WE ARE HAVING THE MODEL 'fit' TO ALL THE IMAGE\n",
    "# VARIATIONS THAT WILL BE CREATED BY OUR ImageDataGenerator\n",
    "# WHICH WE CALLED 'augmentedData'. WE ARE ALSO POINTING\n",
    "# IT TO THE SMALL COLLECTION OF TESTING DATA SO WE CAN\n",
    "# HAVE A MORE ACCURATE VALIDATION SCORE.\n",
    "'''\n",
    "train_session = model.fit_generator(augmentedData.flow(trainX, trainY, batch_size=batch_size),\n",
    "                        validation_data=(testX, testY),\n",
    "                        steps_per_epoch=len(trainX) // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1)\n",
    "\n",
    "# ONE WE ARE DONE, WE CAN SAVE THE MODEL AS FOLLOWS:\n",
    "model.save(\"boxedFace100augmented.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Webcam Test Boxed Faces (or augmented Boxed Faces).py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# STEP 1: CHOOSE THE BOXED MODEL YOU WANT TO TEST\n",
    "#model = load_model(\"boxedFace100.model\")\n",
    "model = load_model(\"boxedFace100augmented.model\")\n",
    "\n",
    "# DEFINE HUMAN READABLE CLASS NAMES, SINCE THE\n",
    "# MODEL PREDICTION WILL RETURN PROBABILITIES FOR\n",
    "# 0 (male) AND 1 (female)\n",
    "classes = [\"man\", \"woman\"]\n",
    "\n",
    "# STEP 2: LOAD OUR FACE DETECTOR\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# STEP 3: START WEBCAM\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "if not webcam.isOpened():\n",
    "    print(\"can not access webcam\")\n",
    "    exit()\n",
    "\n",
    "# CONTINUOUSLY LOOP...\n",
    "while webcam.isOpened():\n",
    "\n",
    "    _, frame = webcam.read()\n",
    "\n",
    "    # OUR MODEL IS EXPECTING A 100x120 IMAGE DATA ARRAY\n",
    "    # SO WE NEED TO CLIP THE CAMERA INPUT (usually 640x480\n",
    "    # BUT WILL DIFFER BY CAMERA) INTO A SLICE THAT SUPPORTS\n",
    "    # A 5/6 ASPECT RATIO (i.e. 100/120). HERE'S HOW WE DO IT:\n",
    "    (h, w, _) = frame.shape\n",
    "    new_width = int(h*.833333)\n",
    "    startX = int((w-new_width)/2)\n",
    "    imgClipped = frame[:, startX:startX+new_width]\n",
    "\n",
    "    gray = cv2.cvtColor(imgClipped, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(100,100) )\n",
    "\n",
    "    for ( x, y, fw, fh) in faces:\n",
    "        cv2.rectangle(imgClipped, (0,0), (w,y), (0,0,0), -1)\n",
    "        cv2.rectangle(imgClipped, (0,0), (x, h), (0,0,0), -1)\n",
    "        cv2.rectangle(imgClipped, (0, y+fh), (w,h), (0,0,0), -1)\n",
    "        cv2.rectangle(imgClipped, (x+fw, 0), (w, h), (0,0,0), -1)\n",
    "\n",
    "    imgTest = cv2.resize(imgClipped, (100, 120))\n",
    "    imgTest = imgTest.astype(\"float\") / 255.0\n",
    "    imgTest = img_to_array(imgTest)\n",
    "    imgTest = np.expand_dims(imgTest, axis=0)\n",
    "\n",
    "    conf = model.predict(imgTest)[0]\n",
    "\n",
    "    idx = np.argmax(conf)\n",
    "    label = classes[idx]\n",
    "\n",
    "    label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n",
    "    cv2.putText(imgClipped, label, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "    cv2.imshow(\"gender detection\", imgClipped)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Results from the second test](images/06_webcam_result.png)\n",
    "\n",
    "*This one is better still, and there are a lot less false positives for males then before. Predictions are more stable too*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
